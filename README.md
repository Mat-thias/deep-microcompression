# ğŸ§  Deep MicroCompression

**Deep MicroCompression** is a research-driven project focused on optimizing deep neural network inference for **resource-constrained microcontrollers**. Inspired by Song Hanâ€™s *Deep Compression*, this project explores the combination of:

* **Structured pruning**
* **Low-bit quantization** (notably 4-bit)
* **Bit-level data packing**
* **Custom deployment kernels in C/C++**

The goal is to **reduce model size, memory access cost, and compute overhead** â€” all without significantly degrading accuracy â€” in environments where every byte and cycle counts.

---

## ğŸ’¡ Core Ideas

* **Bit-Packed Quantization**:
  Instead of naively storing 4-bit weights in 8-bit containers, this project implements **bit-packing**, fitting two weights into a single byte â€” saving up to 50% space compared to standard 8-bit quantization.

* **Hardware-Conscious Design**:
  The deployment engine avoids expensive operations like floating-point division or branching, using **bitwise arithmetic and shift-based unpacking** for speed and predictability.

* **Layer-Level Customization**:
  Each layer (e.g., `Conv2d`, `Linear`, `ReLU`) is independently implemented in both PyTorch and C++, supporting quantization and deployment-specific logic.

* **C++ Runtime for Inference**:
  A minimal inference runtime written in C++ mirrors the Python-defined models and supports different quantization modes using conditional compilation.

---

## ğŸ“ Project Structure Overview

```plaintext
development/         # PyTorch models and layer logic
  â””â”€â”€ layers/        # Custom PyTorch layers (with quantization hooks)
  â””â”€â”€ models/        # Model assembly (e.g., Sequential)
  â””â”€â”€ utils.py       # Utilities for packing/processing

deployment/          # C++ inference engine
  â””â”€â”€ layers/        # C++ versions of Conv, Linear, ReLU, etc.
  â””â”€â”€ models/        # Model containers for deployment

tests/               # Various models (LeNet5, Sine, etc.) in C++ & Python
datasets/            # Input datasets (e.g., MNIST)
```

---

## ğŸ“Œ Current Status

* ğŸ› ï¸ Pruning, quantization and bit-packing implemented in key layers
* ğŸ§ª Testing underway with models like LeNet5
* ğŸ“¤ Conversion from PyTorch to C++ header is partially complete
* âŒ Full end-to-end automation (train â†’ quantize â†’ deploy) is **not yet integrated**

---

## ğŸ¯ Objective

To demonstrate that **custom quantization-aware design + data layout optimization** can significantly boost the deployment efficiency of neural networks on **MCUs**, without relying on large ML runtimes or toolchains like TensorFlow Lite Micro.
