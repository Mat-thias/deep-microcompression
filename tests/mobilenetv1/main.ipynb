{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a4c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import copy\n",
    "import random\n",
    "import os\n",
    "from typing import Tuple\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import PIL \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5339369f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/Documents/Research/deep-microcompression/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# sys.path.append(\"/home/matthias/Documents/EmbeddedAI/deep-microcompression/\")\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from development import (\n",
    "    Sequential,\n",
    "    AvgPool2d,\n",
    "    BatchNorm2d,\n",
    "    Conv2d,\n",
    "    Flatten,\n",
    "    Linear,\n",
    "    ReLU,\n",
    "    ReLU6,\n",
    "    MaxPool2d,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43913a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "mobilenetv1_file = f\"mobilenetv1_state_dict_from_tf.pth\"\n",
    "\n",
    "LUCKY_NUMBER = 25\n",
    "torch.manual_seed(LUCKY_NUMBER)\n",
    "torch.random.manual_seed(LUCKY_NUMBER)\n",
    "torch.cuda.manual_seed(LUCKY_NUMBER)\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b09a8cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ImageNet_Validation_DataSet(data.Dataset):\n",
    "\n",
    "#     def __init__(self, image_dir, combined_label_file, transformer=None):\n",
    "        \n",
    "#         if not os.path.exists(image_dir):\n",
    "#             raise RuntimeError(f\"image_dir {image_dir} doesn't exist!\")\n",
    "        \n",
    "#         if not os.path.exists(combined_label_file):\n",
    "#             raise RuntimeError(f\"combined_label_file {combined_label_file} doesn't exist!\")\n",
    "\n",
    "#         self.image_dir = image_dir\n",
    "#         self.images =  os.listdir(image_dir)\n",
    "#         self.images.sort()\n",
    "\n",
    "#         with open(combined_label_file, \"r\") as file:\n",
    "#             self.labels, self.class_names = list(), list()\n",
    "\n",
    "#             for line in file.readlines()[1:]:\n",
    "#                 _, _, tf_label, class_name = line.strip().split(\", \")\n",
    "#                 self.labels.append(tf_label)\n",
    "#                 self.class_names.append(class_name)\n",
    "\n",
    "#         assert len(self.images) == len(self.labels), \"Images not of the same length as targets\"\n",
    "\n",
    "#         self.transformer = transformer\n",
    "\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.images)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image = PIL.Image.open(os.path.join(self.image_dir, self.images[idx])).convert(\"RGB\")\n",
    "#         if self.transformer:\n",
    "#             image = self.transformer(image)\n",
    "#         return  image, int(self.labels[idx])\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "# imagenet_transformer = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.5, 0.5, 0.50], std=[0.5, 0.5, 0.5])\n",
    "# ])\n",
    "\n",
    "# imagenet_val_dir = \"../../../Datasets/ImageNet_2012/validation/ILSVRC2012_img_val/\"\n",
    "# imagenet_val_combined_label_file = \"../../../Datasets/ImageNet_2012/validation/ILSVRC2012_validation_combined_ground_truth.txt\"\n",
    "# imagenet_val_dataset = ImageNet_Validation_DataSet(image_dir=imagenet_val_dir, combined_label_file=imagenet_val_combined_label_file, transformer=imagenet_transformer)\n",
    "# imagenet_val_dataloader = data.DataLoader(imagenet_val_dataset, shuffle=False, batch_size=32, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f01e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    # transforms.RandomCrop((24, 24)),\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cifar10_train_dataset = datasets.CIFAR10(\"../../../Datassets/\", train=True, download=True, transform=data_transform)\n",
    "cifar10_test_dataset = datasets.CIFAR10(\".../../../Datassets/\", train=False, download=True, transform=data_transform)\n",
    "\n",
    "cifar10_train_loader = data.DataLoader(cifar10_train_dataset, batch_size=32, shuffle=True)\n",
    "cifar10_test_loader = data.DataLoader(cifar10_test_dataset, batch_size=32)\n",
    "\n",
    "# cifar100_train_dataset = datasets.CIFAR100(\"./datasets\", train=True, download=True, transform=data_transform)\n",
    "# cifar100_test_dataset = datasets.CIFAR100(\"./datasets\", train=False, download=True, transform=data_transform)\n",
    "\n",
    "# cifar100_train_loader = data.DataLoader(cifar100_train_dataset, batch_size=32, shuffle=True, num_workers=os.cpu_count())\n",
    "# cifar100_test_loader = data.DataLoader(cifar100_test_dataset, batch_size=32, num_workers=os.cpu_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e5ea6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBatchReLU(\n",
    "        in_channels:int,\n",
    "        out_channels:int,\n",
    "        kernel_size:int,\n",
    "        stride:int = 1,\n",
    "        groups:int = 1,\n",
    "        pad:Tuple[int, int, int, int] = (0, 0, 0, 0),\n",
    "        bias=False,\n",
    "        eps=0.001, \n",
    "        momentum=0.01,\n",
    "):\n",
    "    return (Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, groups=groups, pad=pad, bias=bias),\n",
    "            BatchNorm2d(num_features=out_channels, eps=eps, momentum=momentum, affine=True, track_running_stats=True,),\n",
    "            ReLU6(inplace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b998171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DepthwiseSeperableConv2LUBatchReLU(\n",
    "        in_channels:int,\n",
    "        out_channels:int,\n",
    "        kernel_size:int,\n",
    "        stride:int,\n",
    "        pad:Tuple[int, int, int, int] = (0, 0, 0, 0),\n",
    "        eps=0.001, \n",
    "        momentum=0.01\n",
    "):\n",
    "    return (\n",
    "        *ConvBatchReLU(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size, stride=stride, pad=pad, groups=in_channels, eps=eps, momentum=momentum),\n",
    "        *ConvBatchReLU(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, pad=(0,0,0,0), groups=1, eps=eps, momentum=momentum)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe61828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MobileNetV1(just_backbone=False, classifer=None, pretrained_backbone_weight_file=None):\n",
    "    mobilenetv1_backbone = {\n",
    "        \"conv2d_0\": [3, 32, 3, 2],\n",
    "\n",
    "        \"depthwiseseparable_0\": [32, 64, 3, 1],\n",
    "        \"depthwiseseparable_1\": [64, 128, 3, 2],\n",
    "        \"depthwiseseparable_2\": [128, 128, 3, 1],\n",
    "        \"depthwiseseparable_3\": [128, 256, 3, 2],\n",
    "        \"depthwiseseparable_4\": [256, 256, 3, 1],\n",
    "        \"depthwiseseparable_5\": [256, 512, 3, 2],\n",
    "\n",
    "        \"depthwiseseparable_6\": [512, 512, 3, 1],\n",
    "        \"depthwiseseparable_7\": [512, 512, 3, 1],\n",
    "        \"depthwiseseparable_8\": [512, 512, 3, 1],\n",
    "        \"depthwiseseparable_9\": [512, 512, 3, 1],\n",
    "        \"depthwiseseparable_10\": [512, 512, 3, 1],\n",
    "\n",
    "        \"depthwiseseparable_11\": [512, 1024, 3, 2],\n",
    "        \"depthwiseseparable_12\": [1024, 1024, 3, 1],\n",
    "\n",
    "    }\n",
    "\n",
    "    batchnorm_eps = 0.001\n",
    "    batchnorm_momentum = 1 - 0.99\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    for name, parameters in mobilenetv1_backbone.items():\n",
    "        if \"conv2d\" in name:\n",
    "            in_channels, out_channels, kernel_size, stride = parameters\n",
    "            if stride == 2:\n",
    "                pad = (0, 1, 0, 1)\n",
    "            else:\n",
    "                raise RuntimeError(f\"Unexpected type Conv layer\")\n",
    "            layers.extend(ConvBatchReLU(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, pad=pad, eps=batchnorm_eps, momentum=batchnorm_momentum))\n",
    "        elif \"depthwiseseparable\" in name:\n",
    "            in_channels, out_channels, kernel_size, stride = parameters\n",
    "            if stride == 2:\n",
    "                pad = (0, 1, 0, 1)\n",
    "            else:\n",
    "                pad = tuple([1]*4)\n",
    "            layers.extend(DepthwiseSeperableConv2LUBatchReLU(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, pad=pad, eps=batchnorm_eps, momentum=batchnorm_momentum))\n",
    "        else:\n",
    "            raise ValueError(f\"Recieved unexpected layer of {name}\")\n",
    "    \n",
    "    model_backbone = Sequential(*layers)\n",
    "\n",
    "\n",
    "    if pretrained_backbone_weight_file is not None:\n",
    "        state_dict = torch.load(pretrained_backbone_weight_file)\n",
    "        print(model_backbone.load_state_dict(torch.load(pretrained_backbone_weight_file), strict=False)\n",
    ")\n",
    "    if not just_backbone:\n",
    "        layers = []\n",
    "\n",
    "        if classifer is None:\n",
    "            classifer = {\n",
    "                \"avgpool_0\": [7],\n",
    "                \"conv2d_1\": [1024, 1000, 1, 1],\n",
    "                \"flatten_0\": [],\n",
    "            }\n",
    "\n",
    "        for name, parameters in classifer.items():\n",
    "            if \"conv2d\" in name:\n",
    "                in_channels, out_channels, kernel_size, stride = parameters\n",
    "                layers.extend(ConvBatchReLU(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, eps=batchnorm_eps, momentum=batchnorm_momentum))\n",
    "            elif \"avgpool\" in name:\n",
    "                kernel_size = parameters[0]\n",
    "                stride = kernel_size\n",
    "                layers.append(AvgPool2d(kernel_size=kernel_size))\n",
    "            elif \"flatten\" in name:\n",
    "                layers.append(Flatten())\n",
    "            elif \"linear\" in name:\n",
    "                in_features, out_features = parameters\n",
    "                layers.append(Linear(in_features=in_features, out_features=out_features))\n",
    "            else:\n",
    "                raise ValueError(f\"Recieved unexpected layer of {name}\")\n",
    "\n",
    "    return model_backbone.extend(layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fc7512",
   "metadata": {},
   "source": [
    "# Mobilenetv1 for CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5acd36cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['conv2d_27.weight', 'conv2d_27.bias', 'conv2d_0.bias', 'conv2d_1.bias', 'conv2d_2.bias', 'conv2d_3.bias', 'conv2d_4.bias', 'conv2d_5.bias', 'conv2d_6.bias', 'conv2d_7.bias', 'conv2d_8.bias', 'conv2d_9.bias', 'conv2d_10.bias', 'conv2d_11.bias', 'conv2d_12.bias', 'conv2d_13.bias', 'conv2d_14.bias', 'conv2d_15.bias', 'conv2d_16.bias', 'conv2d_17.bias', 'conv2d_18.bias', 'conv2d_19.bias', 'conv2d_20.bias', 'conv2d_21.bias', 'conv2d_22.bias', 'conv2d_23.bias', 'conv2d_24.bias', 'conv2d_25.bias', 'conv2d_26.bias'])\n"
     ]
    }
   ],
   "source": [
    "cifar10_classifier = {\n",
    "    \"conv2d_1\": [1024, 10, 1, 1],\n",
    "    \"flatten_0\": [],\n",
    "}\n",
    "mobilenetv1_cifar10_model = MobileNetV1(\n",
    "    just_backbone=False, \n",
    "    classifer=cifar10_classifier, \n",
    "    pretrained_backbone_weight_file=mobilenetv1_file\n",
    ").to(DEVICE)\n",
    "\n",
    "\n",
    "# mobilenetv1_cifar10_model.load_state_dict(torch.load(mobilenetv1_file), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5118098b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv2d_0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "  (batchnorm2d_0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_0): ReLU6(inplace=True)\n",
       "  (conv2d_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False)\n",
       "  (batchnorm2d_1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_1): ReLU6(inplace=True)\n",
       "  (conv2d_2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (batchnorm2d_2): BatchNorm2d(64, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_2): ReLU6(inplace=True)\n",
       "  (conv2d_3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), groups=64, bias=False)\n",
       "  (batchnorm2d_3): BatchNorm2d(64, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_3): ReLU6(inplace=True)\n",
       "  (conv2d_4): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (batchnorm2d_4): BatchNorm2d(128, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_4): ReLU6(inplace=True)\n",
       "  (conv2d_5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), groups=128, bias=False)\n",
       "  (batchnorm2d_5): BatchNorm2d(128, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_5): ReLU6(inplace=True)\n",
       "  (conv2d_6): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (batchnorm2d_6): BatchNorm2d(128, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_6): ReLU6(inplace=True)\n",
       "  (conv2d_7): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), groups=128, bias=False)\n",
       "  (batchnorm2d_7): BatchNorm2d(128, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_7): ReLU6(inplace=True)\n",
       "  (conv2d_8): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (batchnorm2d_8): BatchNorm2d(256, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_8): ReLU6(inplace=True)\n",
       "  (conv2d_9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), groups=256, bias=False)\n",
       "  (batchnorm2d_9): BatchNorm2d(256, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_9): ReLU6(inplace=True)\n",
       "  (conv2d_10): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (batchnorm2d_10): BatchNorm2d(256, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_10): ReLU6(inplace=True)\n",
       "  (conv2d_11): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), groups=256, bias=False)\n",
       "  (batchnorm2d_11): BatchNorm2d(256, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_11): ReLU6(inplace=True)\n",
       "  (conv2d_12): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (batchnorm2d_12): BatchNorm2d(512, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_12): ReLU6(inplace=True)\n",
       "  (conv2d_13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "  (batchnorm2d_13): BatchNorm2d(512, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_13): ReLU6(inplace=True)\n",
       "  (conv2d_14): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (batchnorm2d_14): BatchNorm2d(512, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_14): ReLU6(inplace=True)\n",
       "  (conv2d_15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "  (batchnorm2d_15): BatchNorm2d(512, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_15): ReLU6(inplace=True)\n",
       "  (conv2d_16): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (batchnorm2d_16): BatchNorm2d(512, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_16): ReLU6(inplace=True)\n",
       "  (conv2d_17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "  (batchnorm2d_17): BatchNorm2d(512, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_17): ReLU6(inplace=True)\n",
       "  (conv2d_18): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (batchnorm2d_18): BatchNorm2d(512, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_18): ReLU6(inplace=True)\n",
       "  (conv2d_19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "  (batchnorm2d_19): BatchNorm2d(512, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_19): ReLU6(inplace=True)\n",
       "  (conv2d_20): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (batchnorm2d_20): BatchNorm2d(512, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_20): ReLU6(inplace=True)\n",
       "  (conv2d_21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), groups=512, bias=False)\n",
       "  (batchnorm2d_21): BatchNorm2d(512, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_21): ReLU6(inplace=True)\n",
       "  (conv2d_22): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (batchnorm2d_22): BatchNorm2d(512, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_22): ReLU6(inplace=True)\n",
       "  (conv2d_23): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), groups=512, bias=False)\n",
       "  (batchnorm2d_23): BatchNorm2d(512, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_23): ReLU6(inplace=True)\n",
       "  (conv2d_24): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (batchnorm2d_24): BatchNorm2d(1024, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_24): ReLU6(inplace=True)\n",
       "  (conv2d_25): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), groups=1024, bias=False)\n",
       "  (batchnorm2d_25): BatchNorm2d(1024, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_25): ReLU6(inplace=True)\n",
       "  (conv2d_26): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (batchnorm2d_26): BatchNorm2d(1024, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (relu6_26): ReLU6(inplace=True)\n",
       "  (81): Conv2d(1024, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (82): BatchNorm2d(10, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (83): ReLU6(inplace=True)\n",
       "  (84): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobilenetv1_cifar10_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "785804ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(cifar10_test_dataset))[1], next(iter(cifar10_test_dataset))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7be0be2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m top1_acc_fun = \u001b[38;5;28;01mlambda\u001b[39;00m y_pred, y_true: (y_pred.argmax(dim=\u001b[32m1\u001b[39m) == y_true).sum().item()    \n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmobilenetv1_cifar10_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcifar10_test_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop1_acc_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Research/deep-microcompression/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Research/deep-microcompression/tests/mobilenetv1/../../development/models/sequential.py:273\u001b[39m, in \u001b[36mSequential.evaluate\u001b[39m\u001b[34m(self, data_loader, metric_fun, device)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m X, y_true \u001b[38;5;129;01min\u001b[39;00m tqdm(data_loader):\n\u001b[32m    272\u001b[39m     X = X.to(device)\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     y_true = \u001b[43my_true\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m(device)\n\u001b[32m    274\u001b[39m     y_pred = \u001b[38;5;28mself\u001b[39m(X)\n\u001b[32m    275\u001b[39m     metric_val += metric_fun(y_pred, y_true)\n",
      "\u001b[31mAttributeError\u001b[39m: 'int' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "top1_acc_fun = lambda y_pred, y_true: (y_pred.argmax(dim=1) == y_true).sum().item()    \n",
    "mobilenetv1_cifar10_model.evaluate(cifar10_test_dataset, top1_acc_fun, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23db916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1563 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original model accuracy is 68.75% with size 17059232 bytes.\n",
      "tensor([[ 3.2222e+00,  9.0384e+00,  1.6922e+00,  2.9519e+00,  5.3741e+00,\n",
      "          3.3085e-01,  4.1090e+00,  2.5040e+00,  3.1401e+00, -8.7384e-02,\n",
      "         -1.1370e+00, -2.6074e-01, -1.9652e-01, -3.4057e+00, -2.7831e+00,\n",
      "          1.2588e+00, -2.9276e+00, -3.2650e+00, -9.5138e-01, -2.4112e+00,\n",
      "         -8.9809e+00,  9.1203e-01, -1.9521e+00, -2.0736e+00, -3.3935e+00,\n",
      "          3.4715e+00,  6.2112e+00,  7.1531e+00,  1.6990e+00,  1.0570e+01,\n",
      "         -1.5256e+00, -1.4328e+00,  1.4381e+00,  4.5991e+00,  3.1130e+00,\n",
      "          2.0042e+00, -1.5593e+00,  1.9062e+00,  2.0727e+00,  5.6052e+00,\n",
      "          4.3260e+00,  3.1655e-01,  6.4658e+00,  5.5718e+00,  1.6820e+00,\n",
      "          4.5489e+00,  3.1417e+00,  5.6063e+00,  6.7165e+00,  3.1532e+00,\n",
      "          2.3107e+00,  2.3647e+00, -6.0248e-01, -5.0344e+00, -2.5723e+00,\n",
      "         -1.7455e+00,  1.8690e-02, -3.9020e+00, -3.9278e+00, -2.1961e+00,\n",
      "         -8.1626e-01, -5.3579e-01,  2.3169e-01, -1.7479e+00, -1.1757e+00,\n",
      "         -4.4940e-01, -1.5338e+00, -7.3365e-02,  9.8786e-02, -1.7126e+00,\n",
      "         -3.6342e+00,  6.3780e+00, -1.8246e+00,  1.8888e+00,  3.5498e+00,\n",
      "          1.2338e+00,  6.5757e+00,  4.8736e-01,  5.1082e-01,  3.3087e+00,\n",
      "         -3.2387e+00, -3.3708e+00, -3.7643e+00, -1.3923e+00, -2.7345e+00,\n",
      "          2.9209e-01, -3.1470e+00, -1.9287e+00,  3.4796e+00, -4.7775e-01,\n",
      "         -2.3668e+00, -3.7594e+00,  3.0451e-01, -1.6263e+00, -9.2581e-01,\n",
      "         -2.6296e+00, -4.2159e+00, -5.1374e+00, -3.7772e+00, -1.2251e+00,\n",
      "          7.0472e-01,  4.0153e+00, -6.5221e-01,  1.4689e+00,  3.6048e+00,\n",
      "          7.1888e-01,  8.5261e-01,  2.9397e+00,  4.5409e+00,  9.5781e-01,\n",
      "         -1.9007e+00,  1.6350e+00,  4.6668e+00,  1.8701e+00,  2.0527e+00,\n",
      "          6.9114e+00, -1.0765e+00,  4.6986e-01, -1.6416e-01,  1.3683e+00,\n",
      "          3.6510e+00,  2.8560e+00,  2.1723e+00, -3.8452e-02,  2.7195e+00,\n",
      "         -1.2306e+00,  6.6410e-01,  1.9933e+00, -1.8349e+00,  2.3408e+00,\n",
      "          3.4796e+00, -7.0925e+00, -9.1973e-01, -6.6421e-01, -2.0922e+00,\n",
      "         -1.6844e+00, -4.7307e+00, -5.9037e+00, -1.9441e+00, -7.8615e+00,\n",
      "         -4.2233e+00, -2.7971e+00, -2.9773e+00, -5.9680e+00,  1.4246e+00,\n",
      "         -7.8831e-01, -1.6209e+00, -2.9458e+00, -2.5283e+00,  8.2347e-01,\n",
      "          6.3269e+00,  3.6153e+00, -1.7988e+00,  4.1659e-01, -5.7917e-01,\n",
      "         -1.8009e+00,  2.4417e+00,  1.0674e+00,  2.6146e+00, -1.4244e+00,\n",
      "         -3.8011e+00,  2.0503e+00,  2.9092e+00, -2.8114e+00, -3.5332e+00,\n",
      "         -1.4578e+00, -2.2443e-01, -4.7118e-01,  3.4523e+00,  2.0218e+00,\n",
      "         -1.1543e+00,  3.2115e+00,  1.3230e+00,  2.6006e+00, -1.9063e+00,\n",
      "         -1.4216e-01,  1.6706e-01,  1.5200e+00,  2.4276e+00,  1.1984e+00,\n",
      "          2.2070e+00,  3.4964e-01,  1.3016e+00, -2.0939e+00,  3.5056e+00,\n",
      "          5.4013e-01,  5.1650e-01, -4.5321e+00,  3.0384e-01,  2.1878e+00,\n",
      "         -1.3364e+00,  3.5561e+00,  3.1974e-01, -8.3303e-01, -2.9316e+00,\n",
      "          1.1593e+00, -1.5048e+00, -1.8811e-01, -3.1314e+00, -1.7289e+00,\n",
      "         -2.1123e+00, -1.1214e+00, -3.7795e+00,  3.9270e+00, -9.0828e-03,\n",
      "         -3.1471e+00, -5.9364e-01,  4.7094e+00,  2.2464e+00,  1.9702e+00,\n",
      "         -4.0008e+00,  2.4488e+00,  4.0512e+00,  3.8138e+00, -2.1138e+00,\n",
      "          1.7758e+00,  7.3530e-01, -2.8510e-01, -5.8103e-01, -2.4014e+00,\n",
      "         -2.7043e+00, -2.1674e+00,  2.6774e+00, -1.0525e+00,  3.4695e-01,\n",
      "         -3.1870e+00, -1.8095e+00, -1.9563e+00,  1.3723e+00,  6.2534e-01,\n",
      "          2.0814e-01, -1.1101e+00, -4.4147e+00, -8.8707e-01,  8.8191e-01,\n",
      "         -4.5412e-01,  5.0295e-01,  1.7756e+00,  1.5836e+00, -6.2447e-01,\n",
      "         -6.2224e-01, -1.2539e+00, -3.7860e-01, -2.1852e+00, -5.3563e-01,\n",
      "          2.0155e+00,  8.0906e-01, -2.2642e+00, -5.2808e-01, -1.1157e+00,\n",
      "         -6.0387e-01, -5.3236e-01,  1.3740e+00,  5.0282e+00,  1.0652e+00,\n",
      "         -2.7151e+00, -5.5122e+00,  1.7356e+00,  3.3753e+00,  3.6625e+00,\n",
      "          1.4073e+00, -3.4694e+00,  3.7893e+00, -1.0991e+00,  1.1707e+00,\n",
      "          7.7043e-01, -1.6366e-01,  2.0214e+00,  3.5080e+00, -1.3742e+00,\n",
      "         -1.0263e+00,  6.1764e+00,  3.4674e+00,  5.6491e+00,  4.2174e+00,\n",
      "          1.9098e+00,  5.5003e+00,  3.1538e+00,  5.3570e+00,  1.1456e+00,\n",
      "          7.3040e+00,  1.7288e+00,  9.3843e-01,  9.1101e-01, -1.2982e+00,\n",
      "          2.7738e+00,  2.0090e+00, -3.1283e+00,  6.1349e-01, -1.1502e+00,\n",
      "         -1.0969e+00,  6.2493e+00, -2.4189e+00,  8.2079e-01, -1.5882e+00,\n",
      "         -5.2299e+00,  1.3446e+00, -8.9278e-01,  5.6051e+00,  5.5678e+00,\n",
      "         -7.1926e+00, -1.0494e+00, -2.2945e+00, -1.5434e+00, -4.2325e+00,\n",
      "          1.0738e+00,  1.4065e+00, -1.6832e+00, -2.7122e+00, -1.7622e+00,\n",
      "         -1.3450e-01,  1.3997e+00,  8.1308e-01,  2.4370e-01,  6.6465e-01,\n",
      "         -3.0171e+00, -4.4408e+00, -2.5512e+00, -2.8680e-01,  2.7385e-01,\n",
      "         -3.8754e+00, -4.7691e+00, -6.2767e+00, -5.8096e+00, -3.1263e+00,\n",
      "          7.8660e-02, -4.1642e+00,  7.9085e+00,  1.1173e+00,  4.4452e+00,\n",
      "          2.8750e+00,  2.1876e+00,  1.4148e+00,  2.4972e+00,  1.4515e+00,\n",
      "          1.0053e+00, -6.2791e-01, -4.3328e-01,  2.1086e+00,  3.1857e+00,\n",
      "          2.8647e+00,  8.3429e+00,  7.4884e+00,  7.2699e+00,  2.7750e+00,\n",
      "         -8.5841e-01,  1.9635e+00, -2.5516e-01, -2.5788e+00, -1.2996e+00,\n",
      "         -2.1103e+00,  1.2238e+00,  4.8694e-01,  8.2208e-01,  2.9275e+00,\n",
      "          1.6167e+00,  1.9171e+00,  4.8105e-01,  2.3308e+00,  1.4591e+00,\n",
      "          1.6110e+00,  7.5168e-01,  3.1238e+00,  2.9756e+00, -2.5228e+00,\n",
      "          1.2468e+00, -3.3053e+00, -2.1517e+00, -1.5242e+00, -2.5912e+00,\n",
      "          7.2554e-01,  2.9143e+00,  4.5987e+00,  1.4939e+00,  3.5996e+00,\n",
      "         -2.0647e+00, -8.5889e-01,  2.7266e+00,  1.4629e+00,  8.2757e-01,\n",
      "          9.1140e-01,  5.5635e+00,  4.1637e+00,  1.9707e-01, -1.2081e+00,\n",
      "          3.3670e+00, -6.7612e-02,  4.8433e-02, -1.2575e+00,  5.1349e+00,\n",
      "          2.4012e+00,  1.9872e+00, -1.7085e+00, -4.3605e-01,  7.7226e-01,\n",
      "          1.3635e+00,  1.3290e+00, -7.9199e-01, -4.2177e+00, -6.2168e+00,\n",
      "         -1.2855e+00, -1.8089e-01,  9.9170e-01, -5.5901e+00, -3.2825e+00,\n",
      "         -2.6752e+00,  1.4882e+00,  1.0521e+00, -5.0403e+00,  6.3261e+00,\n",
      "          3.0479e-01, -2.8404e+00,  4.5292e+00,  2.3717e+00, -9.6649e-01,\n",
      "         -2.7485e+00, -1.4452e+00, -2.7000e+00, -2.0749e+00,  3.5590e+00,\n",
      "          4.1741e+00,  1.0278e+00, -1.2689e+00, -3.2179e+00, -8.6680e-01,\n",
      "          1.1974e+00,  2.8877e+00,  2.8716e+00,  3.8933e+00,  2.8067e-01,\n",
      "         -3.7857e+00, -5.2112e-01,  9.0424e-01,  1.9984e+00, -9.6395e-01,\n",
      "          4.2668e+00, -3.6286e+00,  8.9127e-01, -2.1929e+00, -8.7830e-01,\n",
      "          2.8123e+00,  8.3943e-01, -8.9661e-02, -1.9686e+00, -3.7685e+00,\n",
      "         -8.5894e-01,  7.9423e-02, -3.3359e+00, -2.6255e+00, -3.3394e+00,\n",
      "         -4.4766e+00, -2.7490e+00, -1.3532e+00,  1.9885e-02, -2.4245e-01,\n",
      "         -1.9935e+00, -2.0642e+00,  3.2225e-01, -4.3133e-02,  1.3818e+00,\n",
      "         -3.4625e+00,  1.2767e+00,  1.4283e+00,  3.2193e+00, -1.9565e+00,\n",
      "         -5.4266e+00, -7.3397e+00,  2.8283e+00, -2.3165e+00,  5.5777e+00,\n",
      "          6.0530e+00,  3.3567e-01, -9.7721e-01, -3.6393e+00, -2.9245e+00,\n",
      "          5.0441e-01, -2.3165e+00, -7.2987e-02, -7.3094e-01, -1.7475e+00,\n",
      "         -2.9254e-01, -2.6830e+00, -3.8205e+00,  1.7173e-01, -2.5831e-02,\n",
      "         -4.6654e+00,  3.3168e+00,  7.9400e-01,  2.2952e+00,  3.3780e+00,\n",
      "         -7.4328e-01,  5.4427e-01, -5.5961e-01,  1.4899e+00, -2.7684e+00,\n",
      "         -8.8273e-01, -2.3702e+00,  1.9310e-02,  1.2515e+00, -1.8013e-01,\n",
      "          2.8557e+00, -5.2306e-01,  1.9019e+00, -2.5397e+00, -7.6161e-02,\n",
      "         -7.6300e-01, -3.8820e+00, -1.2739e+00, -2.2581e+00, -1.8930e+00,\n",
      "         -3.4065e+00, -2.6086e+00, -2.7106e+00,  8.5735e-01,  1.0524e+00,\n",
      "         -1.0828e+00, -4.6712e-01, -5.8582e-01,  1.2868e-01, -4.7753e-01,\n",
      "         -7.7128e-01, -6.7912e-01, -7.0611e-01, -7.3764e-01,  5.8431e-01,\n",
      "         -6.3082e+00, -1.3526e+00, -2.3007e+00, -2.5398e+00,  2.0801e+00,\n",
      "          3.3481e+00,  2.1508e+00, -1.5675e+00, -4.9772e+00, -5.4788e+00,\n",
      "         -7.2975e+00, -3.1996e+00, -3.6140e+00, -2.2746e+00, -1.3857e+00,\n",
      "         -1.3167e+00,  2.8058e+00,  2.8070e+00,  3.9264e-01,  3.6248e+00,\n",
      "         -8.4253e-01,  1.5452e+00, -4.3030e+00, -3.6850e+00, -5.9260e-01,\n",
      "         -5.7108e+00, -2.5988e+00,  7.4301e+00, -1.8163e-01, -4.3483e+00,\n",
      "         -7.7994e-01,  9.1331e+00,  3.3294e-01,  3.9404e+00, -2.7633e-01,\n",
      "         -1.2856e+00, -6.1906e-01, -8.7378e-01, -2.3221e+00,  1.9323e+00,\n",
      "         -3.3132e+00,  1.5044e+00, -1.6923e+00, -1.2479e+00, -4.1461e+00,\n",
      "          4.7996e+00,  1.1777e+00, -2.4687e+00, -5.6158e+00, -1.0331e+00,\n",
      "         -5.4982e-01, -2.8550e-01,  2.9730e+00, -1.1337e+00,  8.9890e-01,\n",
      "          2.4382e+00, -2.2285e+00, -4.9521e-02, -1.4008e+00,  9.1305e-01,\n",
      "         -7.9544e-01, -2.6878e+00,  6.6011e-01,  1.0819e+00,  3.5344e-01,\n",
      "         -5.4926e+00, -6.3619e-01, -5.3149e+00, -1.2558e-01,  2.3368e-01,\n",
      "         -1.6643e+00, -1.0530e+00, -5.4599e-01, -1.5817e+00, -2.8894e+00,\n",
      "          6.1669e-01,  1.6050e+00,  6.3415e-01,  1.0659e+00, -2.0446e+00,\n",
      "         -1.4611e+00, -2.9942e+00,  6.6968e+00,  5.2164e-01, -3.1587e-02,\n",
      "         -3.4892e-01,  1.9211e-01, -1.9674e+00, -4.6357e+00, -2.3677e+00,\n",
      "         -5.4074e+00,  1.8344e+00,  9.6170e-01, -3.9617e-01,  8.1670e-01,\n",
      "         -1.0872e+00, -1.6518e+00, -1.5390e+00, -2.0243e+00, -3.2140e+00,\n",
      "         -5.4103e+00,  4.1455e+00, -3.7655e+00, -1.7731e+00,  1.1468e+00,\n",
      "         -2.6796e+00,  1.6250e-01, -3.6308e+00, -5.5240e-01,  7.4397e-02,\n",
      "         -1.1057e+00,  1.2105e+00,  1.8419e+00,  1.0713e+00, -3.6889e-01,\n",
      "         -2.2019e+00,  4.5003e-01, -2.1235e+00,  3.8847e+00,  4.4177e+00,\n",
      "         -4.0684e-01,  1.6841e+00, -3.2196e+00, -1.8369e+00,  2.5558e+00,\n",
      "          2.8664e+00,  1.4810e-01, -3.7345e+00,  1.2965e+00, -1.2300e+00,\n",
      "          2.1601e+00, -3.3305e+00,  2.0038e+00, -1.3545e+00, -1.6894e+00,\n",
      "         -6.6889e-01, -4.6611e+00, -2.5524e+00, -2.3928e+00, -1.3505e+00,\n",
      "          1.4239e+00,  2.1000e+00, -3.0832e+00,  1.0872e+00,  5.2720e+00,\n",
      "          1.9115e+00,  2.4893e+00,  3.7490e-01, -1.6391e+00,  6.7497e-01,\n",
      "         -3.0867e+00,  1.1702e+00,  1.9163e+00, -7.7548e-01, -3.0857e+00,\n",
      "          2.5381e+00, -8.5476e-01,  3.6063e+00,  1.8548e+00,  1.7094e-01,\n",
      "         -1.5903e+00, -3.6621e+00, -2.5312e+00,  3.7071e+00, -3.9203e+00,\n",
      "          1.3728e-01, -3.2852e+00,  2.3435e+00, -1.1603e+00, -2.8645e+00,\n",
      "          4.4877e-01,  1.7692e-01, -9.7097e-01, -1.3150e+00, -1.0170e+00,\n",
      "         -9.3259e-01, -1.5713e+00,  2.9289e+00,  1.6421e+00,  1.7515e+00,\n",
      "         -4.5251e+00, -2.6755e+00, -1.5691e+00, -2.1882e+00, -9.2384e-01,\n",
      "         -8.4424e-02, -9.0802e-01, -8.5489e-01, -1.0867e+00, -5.0849e+00,\n",
      "          1.1045e+00,  4.0228e+00, -3.1652e+00, -3.0198e+00,  1.6850e+00,\n",
      "          1.1532e+00,  9.6055e-01,  1.5579e+00, -2.3708e+00, -4.5179e+00,\n",
      "         -3.1016e-01, -3.9643e+00, -4.0523e+00,  3.9388e+00, -1.8615e+00,\n",
      "          2.6030e+00,  1.7940e+00, -5.0020e+00,  2.3436e+00, -4.2318e+00,\n",
      "         -1.9148e+00, -4.0119e-01, -1.1055e+00,  2.4356e+00,  2.3787e+00,\n",
      "         -1.2336e+00, -7.7445e-01, -1.4372e+00, -6.5320e-01,  1.3295e+00,\n",
      "         -2.2721e+00, -2.3212e+00, -9.2201e-01, -5.4151e+00,  4.1324e+00,\n",
      "         -1.8578e+00, -1.5378e+00, -4.4878e+00,  1.4662e+00, -1.7595e+00,\n",
      "         -7.1085e-01, -1.9385e+00, -1.8465e+00,  6.8787e-01,  5.9877e-01,\n",
      "          8.2557e-01,  5.7014e-01, -7.2763e-01, -4.8233e-01,  5.0037e+00,\n",
      "         -7.1220e-01,  5.0098e+00,  3.2073e-01,  9.0481e-02,  4.0619e+00,\n",
      "         -1.1140e+00, -3.2549e+00,  3.5655e+00, -1.8860e+00, -2.9762e-01,\n",
      "          2.5336e+00,  2.0767e-01, -2.7478e-01,  2.4501e+00, -2.6865e+00,\n",
      "         -4.2991e+00, -3.5924e+00,  5.0044e-01,  1.8657e+00,  3.2963e-01,\n",
      "          1.1471e-01, -2.1516e+00,  7.1946e-01, -3.9634e-01, -3.1141e+00,\n",
      "         -1.0847e+00,  5.1474e-01,  5.4195e+00,  7.8833e-01,  2.4086e+00,\n",
      "          3.9855e-01, -1.0583e+00, -1.5654e+00, -5.7620e+00, -5.2451e-01,\n",
      "         -2.5021e+00,  4.0643e+00, -2.1369e-02,  1.6448e+00, -6.7479e+00,\n",
      "          1.6826e+00,  3.5768e-02, -2.6579e+00,  2.3118e-01, -3.7094e-01,\n",
      "         -1.9291e+00,  3.0971e+00, -2.1667e+00,  1.8307e+00, -7.7931e+00,\n",
      "          1.1507e+00, -4.0520e+00, -2.4181e+00,  4.9297e+00,  3.1712e+00,\n",
      "         -3.6823e+00, -4.4127e+00, -3.9858e-01, -1.7006e-01, -4.9917e+00,\n",
      "         -2.3637e+00, -7.6604e-01,  2.5771e+00, -2.7635e+00, -5.1257e+00,\n",
      "          2.1268e+00, -3.2996e-01,  1.2976e+00, -5.2326e+00, -3.3617e+00,\n",
      "          5.7272e-01, -7.7535e-01,  8.4183e-01,  1.8616e+00, -1.6679e-01,\n",
      "          1.1969e+00,  7.1211e-01,  2.7422e+00,  3.0718e+00, -4.0532e-01,\n",
      "          3.1437e-01, -6.3119e-01, -2.8645e-01, -2.4124e+00, -2.1294e+00,\n",
      "         -4.9829e-01,  3.0408e-01, -1.0591e-02,  7.7091e-01,  1.7405e+00,\n",
      "         -4.6738e+00,  9.1127e-03,  1.1803e+00,  4.3620e-01,  9.3538e-01,\n",
      "         -1.1733e+00, -2.6993e-01,  7.2587e+00,  1.2483e+00,  1.8819e+00,\n",
      "         -2.2994e+00, -2.3856e-01, -2.5656e+00, -9.9340e-01, -1.8435e+00,\n",
      "          1.1663e-01, -2.2862e+00,  1.4733e+00, -9.5008e-01, -1.8766e+00,\n",
      "          2.1868e+00,  2.9047e+00,  1.2266e+00, -4.3639e+00,  5.9447e-01,\n",
      "          7.6534e-01, -4.0066e+00,  2.4331e-01,  1.1992e+00,  4.9645e+00,\n",
      "         -2.0731e+00, -3.4105e-01, -5.2835e-01, -1.5548e-01,  2.5994e+00,\n",
      "          3.2235e+00, -3.3331e+00,  2.2370e+00, -1.6410e+00, -2.9467e+00,\n",
      "         -1.0079e-01, -1.6599e+00,  6.6831e-01, -1.7175e+00,  5.4911e-02,\n",
      "         -3.4509e+00, -2.0983e+00,  3.4458e+00,  2.0621e+00,  3.7894e-01,\n",
      "          1.2848e+00, -3.7826e+00,  1.1748e+00, -3.1802e+00,  3.5616e+00,\n",
      "          1.5947e+00, -4.0375e+00, -2.5977e+00,  3.4623e+00, -5.3404e+00,\n",
      "         -6.7154e-01, -1.3058e+00,  2.4792e-01,  7.5904e-01,  1.1204e+00,\n",
      "          8.4232e-01,  2.9743e+00,  6.0491e-01, -7.5498e-02, -1.2723e+00,\n",
      "         -1.6595e+00, -5.8211e-01, -3.4558e+00, -2.7137e+00,  3.8610e+00,\n",
      "          2.0785e+00,  9.3402e-01,  1.6389e+00, -1.8899e+00,  2.2616e-01,\n",
      "          1.4407e+00, -2.0026e-01, -2.3598e+00, -9.3401e-01, -2.6432e+00,\n",
      "          3.8277e+00, -1.6541e+00,  3.9664e+00, -1.1546e+00, -3.1430e+00,\n",
      "         -1.1143e-01, -2.6395e+00,  1.2619e+00,  1.5395e+00, -2.2643e+00,\n",
      "          1.0321e+00, -1.9371e+00, -2.0527e+00,  1.2873e+00,  2.7714e+00,\n",
      "         -2.1978e+00, -3.9005e+00,  2.8910e+00,  3.8789e+00, -1.3744e+00,\n",
      "         -5.8010e-01, -1.6995e-01, -6.6519e-02, -8.5458e-01,  3.3975e-02,\n",
      "         -2.3773e+00, -2.7258e+00, -6.7252e+00,  9.3761e-01, -1.0576e+00,\n",
      "          2.2818e+00, -1.9746e+00, -2.0938e+00,  1.2337e+00,  1.5691e+00,\n",
      "         -6.3177e-01, -2.3142e+00,  2.2346e+00,  4.2248e-01, -4.4099e+00,\n",
      "          1.0229e+01, -2.5626e+00,  4.4055e+00,  4.9343e+00, -3.8347e-01,\n",
      "         -2.6354e-02,  9.0777e-01,  9.6113e-01, -7.6310e-01,  1.7196e+00,\n",
      "          2.5311e+00,  1.2496e+00, -1.3167e+00, -2.2778e+00,  3.4474e+00,\n",
      "          1.1670e+00, -3.7381e+00, -1.2626e+00,  4.4608e+00, -5.3764e-01]])\n"
     ]
    }
   ],
   "source": [
    "mobilenetv1_model.cpu()\n",
    "\n",
    "mobilenetv1_mcu_model = copy.deepcopy(mobilenetv1_model)\n",
    "\n",
    "original_acc = mobilenetv1_mcu_model.evaluate(imagenet_val_dataloader, top1_acc_fun)\n",
    "original_size = mobilenetv1_mcu_model.get_size_in_bits()//8\n",
    "print(f\"The original model accuracy is {original_acc*100:.2f}% with size {original_size} bytes.\")\n",
    "test_output = mobilenetv1_mcu_model.test(); print(test_output)\n",
    "# mobilenetv1_mcu_model.convert_to_c(var_name=\"mobilenetv1_mcu_model\", src_dir=\"./Arduino Nano 33 BLE/src/\", include_dir=\"./Arduino Nano 33 BLE/include/\")\n",
    "mobilenetv1_mcu_model.convert_to_c(var_name=\"mobilenetv1_mcu_model\", src_dir=\"./HP HP Pavilion Laptop 15-cs3xxx/src/\", include_dir=\"./HP HP Pavilion Laptop 15-cs3xxx/include/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da6789a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.2222,  9.0384,  1.6922,  2.9519,  5.3741,  0.3308,  4.1090,  2.5040,\n",
       "         3.1401, -0.0874, -1.1370, -0.2607, -0.1965, -3.4057, -2.7831,  1.2588,\n",
       "        -2.9276, -3.2650, -0.9514, -2.4112])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "test_output[0, i:i+20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6307e801",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cifar10_test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m mobilenetv1_mcu_model = mobilenetv1_model.prune_channel(sparsity)\n\u001b[32m      6\u001b[39m mobilenetv1_mcu_model.to(DEVICE)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPruned with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msparsity\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, acc = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmobilenetv1_mcu_model.evaluate(\u001b[43mcifar10_test_loader\u001b[49m,\u001b[38;5;250m \u001b[39mtop1_acc_fun,\u001b[38;5;250m \u001b[39mdevice=DEVICE)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m mobilenetv1_mcu_model.cpu()\n\u001b[32m     10\u001b[39m mobilenetv1_mcu_model.convert_to_c(var_name=\u001b[33m\"\u001b[39m\u001b[33mmobilenetv1_mcu_model\u001b[39m\u001b[33m\"\u001b[39m, src_dir=\u001b[33m\"\u001b[39m\u001b[33m./HP HP Pavilion Laptop 15-cs3xxx/src/\u001b[39m\u001b[33m\"\u001b[39m, include_dir=\u001b[33m\"\u001b[39m\u001b[33m./HP HP Pavilion Laptop 15-cs3xxx/include/\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'cifar10_test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "sparsity = .34\n",
    "\n",
    "mobilenetv1_model.cpu()\n",
    "mobilenetv1_mcu_model = mobilenetv1_model.prune_channel(sparsity)\n",
    "\n",
    "mobilenetv1_mcu_model.to(DEVICE)\n",
    "print(f\"Pruned with {sparsity}, acc = {mobilenetv1_mcu_model.evaluate(cifar10_test_loader, top1_acc_fun, device=DEVICE)}\")\n",
    "\n",
    "mobilenetv1_mcu_model.cpu()\n",
    "mobilenetv1_mcu_model.convert_to_c(var_name=\"mobilenetv1_mcu_model\", src_dir=\"./HP HP Pavilion Laptop 15-cs3xxx/src/\", include_dir=\"./HP HP Pavilion Laptop 15-cs3xxx/include/\")\n",
    "\n",
    "mobilenetv1_mcu_model.to(DEVICE)\n",
    "mobilenetv1_mcu_model.test(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3238f2",
   "metadata": {},
   "source": [
    "# MobileNetV1 Tensorflow to Torch Converter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98babdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def copy_tensor(tensor_source, tensor_destination):\n",
    "    tensor_destination.copy_(tensor_source)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def convert_mobilenetv1_tf_to_torch():\n",
    "    mobilenetv1_tf_model = tf.keras.applications.mobilenet.MobileNet(weights=\"imagenet\")\n",
    "    \n",
    "    torch_layers = []\n",
    "    for layer in mobilenetv1_tf_model.layers:\n",
    "\n",
    "\n",
    "        if isinstance(layer, tf.keras.layers.InputLayer):\n",
    "            pass\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.Conv2D):\n",
    "\n",
    "            weight = np.transpose(layer.weights[0], (3, 2, 0, 1))\n",
    "            out_channels, in_channels, kernel_size, _ = weight.shape\n",
    "            stride =layer.strides[0]\n",
    "\n",
    "            if kernel_size == 3 and stride == 2:\n",
    "                pad = (0, 1, 0, 1)\n",
    "                padding = 0\n",
    "            elif kernel_size == 1 and stride == 1:\n",
    "                pad = ()\n",
    "                padding = 0\n",
    "            else:\n",
    "                raise RuntimeError(f\"Unexpected type Conv layer\")\n",
    "            \n",
    "            if len(layer.weights) == 1:\n",
    "                conv_layer = Conv2d(\n",
    "                    in_channels=in_channels, \n",
    "                    out_channels=out_channels, \n",
    "                    kernel_size=kernel_size, \n",
    "                    stride=stride, \n",
    "                    padding=padding, \n",
    "                    pad=pad, \n",
    "                    bias=True\n",
    "                )\n",
    "            else:\n",
    "                conv_layer = Conv2d(\n",
    "                    in_channels=in_channels, \n",
    "                    out_channels=out_channels, \n",
    "                    kernel_size=kernel_size, \n",
    "                    stride=stride, \n",
    "                    padding=padding, \n",
    "                    pad=pad, \n",
    "                    bias=False\n",
    "                )\n",
    "                copy_tensor(torch.from_numpy(layer.weights[1].numpy()), conv_layer.bias)\n",
    "\n",
    "            copy_tensor(torch.from_numpy(weight), conv_layer.weight)\n",
    "            torch_layers.append(conv_layer)         \n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.DepthwiseConv2D):\n",
    "\n",
    "            # Convert to PyTorch format: [in_channels, 1, H, W]\n",
    "            weight = np.transpose(layer.weights[0], (2, 3, 0, 1))\n",
    "            in_channels, depth_multiplier, kernel_size, _ = weight.shape\n",
    "            stride =layer.strides[0]\n",
    "\n",
    "            assert depth_multiplier == 1, \"Depth multiplier must be 1 for depthwise convolutions\"\n",
    "\n",
    "            if kernel_size == 3 and stride == 2:\n",
    "                padding = 0\n",
    "                pad = (0, 1, 0, 1)\n",
    "            elif kernel_size == 3 and stride == 1:\n",
    "                padding = 1\n",
    "                pad = ()\n",
    "            else:\n",
    "                raise RuntimeError(f\"Unexpected type Conv layer\")\n",
    "\n",
    "            if len(layer.weights) == 1:\n",
    "                deepseperableconv_layer = Conv2d(in_channels=in_channels, \n",
    "                    out_channels=in_channels, \n",
    "                    kernel_size=kernel_size, \n",
    "                    stride=stride, \n",
    "                    padding=padding, pad=pad, \n",
    "                    groups=in_channels, \n",
    "                    bias=False\n",
    "                )\n",
    "            else: \n",
    "                deepseperableconv_layer = Conv2d(in_channels=in_channels, \n",
    "                    out_channels=in_channels, \n",
    "                    kernel_size=kernel_size, \n",
    "                    stride=stride, \n",
    "                    padding=padding, pad=pad, \n",
    "                    groups=in_channels, \n",
    "                    bias=True\n",
    "                )\n",
    "                copy_tensor(torch.from_numpy(layer.weights[1].numpy()), deepseperableconv_layer.bias)\n",
    "\n",
    "            copy_tensor(torch.from_numpy(weight), deepseperableconv_layer.weight)\n",
    "\n",
    "            torch_layers.append(deepseperableconv_layer)\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.ReLU):\n",
    "            # torch_layers.append(ReLU())\n",
    "            torch_layers.append(ReLU6())\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            gamma, beta, mean, var = layer.weights\n",
    "            out_channels = gamma.shape[0]\n",
    "\n",
    "            torch_layers.append(BatchNorm2d(out_channels, eps=layer.epsilon, momentum=1 - layer.momentum, affine=layer.center and layer.scale,  track_running_stats=True,))\n",
    "            print(torch_layers[-1].momentum)  \n",
    "            print(layer.epsilon, layer.momentum, layer.center, layer.scale)\n",
    "            copy_tensor(torch.from_numpy(gamma.numpy()), torch_layers[-1].weight)\n",
    "            copy_tensor(torch.from_numpy(beta.numpy()), torch_layers[-1].bias)\n",
    "            copy_tensor(torch.from_numpy(mean.numpy()), torch_layers[-1].running_mean)\n",
    "            copy_tensor(torch.from_numpy(var.numpy()), torch_layers[-1].running_var)\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.MaxPooling2D):\n",
    "            pass\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.GlobalAveragePooling2D):\n",
    "            torch_layers.append(AvgPool2d(kernel_size=7))\n",
    "            pass\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.ZeroPadding2D):\n",
    "            print(\"Not Needed\")\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.Dropout):\n",
    "            print(\"Not Needed\")\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.Reshape):\n",
    "            torch_layers.append(Flatten())\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.Activation):\n",
    "            print(layer.activation.__name__)\n",
    "            torch_layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "        else: \n",
    "            raise RuntimeError(f\"Unknown layer type: {type(layer)}\")\n",
    "        \n",
    "    return Sequential(*torch_layers)\n",
    "    \n",
    "mobilenetv1_torch_model = convert_mobilenetv1_tf_to_torch()\n",
    "type(mobilenetv1_torch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b47d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
