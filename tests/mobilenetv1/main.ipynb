{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a4c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import copy\n",
    "import random\n",
    "import os\n",
    "from typing import Tuple\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import PIL \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "# import tensorflow as tf\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5339369f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/Documents/EmbeddedAI/deep-microcompression/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# sys.path.append(\"/home/matthias/Documents/EmbeddedAI/deep-microcompression/\")\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from development import (\n",
    "    Sequential,\n",
    "    AvgPool2d,\n",
    "    BatchNorm2d,\n",
    "    Conv2d,\n",
    "    Flatten,\n",
    "    Linear,\n",
    "    ReLU6,\n",
    "\n",
    "    EarlyStopper,\n",
    "\n",
    "    QuantizationGranularity,\n",
    "    QuantizationScheme\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43913a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "mobilenetv1_file = f\"mobilenetv1_state_dict_from_tf_{DEVICE}.pth\"\n",
    "mobilenetv1_state_dict_dmc_original_from_tf = f\"mobilenetv1_state_dict_dmc_original_from_tf.pth\"\n",
    "\n",
    "input_shape = (3, 224, 224)\n",
    "LUCKY_NUMBER = 25\n",
    "torch.manual_seed(LUCKY_NUMBER)\n",
    "torch.random.manual_seed(LUCKY_NUMBER)\n",
    "torch.cuda.manual_seed(LUCKY_NUMBER)\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f54f7db",
   "metadata": {},
   "source": [
    "## Getting the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b09a8cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageNet_Validation_DataSet(data.Dataset):\n",
    "\n",
    "    def __init__(self, image_dir, combined_label_file, transformer=None):\n",
    "        \n",
    "        assert os.path.exists(image_dir), f\"image_dir {image_dir} doesn't exist\"\n",
    "        assert os.path.exists(combined_label_file), f\"combined_label_file {combined_label_file} doesn't exist!\"\n",
    "\n",
    "        self.image_dir = image_dir\n",
    "        self.images =  os.listdir(image_dir)\n",
    "        self.images.sort()\n",
    "\n",
    "        with open(combined_label_file, \"r\") as file:\n",
    "            self.labels, self.class_names = list(), list()\n",
    "            for line in file.readlines()[1:]:\n",
    "                _, _, tf_label, class_name = line.strip().split(\", \")\n",
    "                self.labels.append(tf_label)\n",
    "                self.class_names.append(class_name)\n",
    "\n",
    "        assert len(self.images) == len(self.labels), \"Images not of the same length as targets\"\n",
    "        self.transformer = transformer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = PIL.Image.open(os.path.join(self.image_dir, self.images[idx])).convert(\"RGB\")\n",
    "        if self.transformer:\n",
    "            image = self.transformer(image)\n",
    "        return  image, int(self.labels[idx])\n",
    "        \n",
    "imagenet_transformer = transforms.Compose([\n",
    "    transforms.Resize(input_shape[1:]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.50], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "imagenet_val_dir = \"../../../Datasets/ImageNet_2012/validation/ILSVRC2012_img_val/\"\n",
    "imagenet_val_combined_label_file = \"../../../Datasets/ImageNet_2012/validation/ILSVRC2012_validation_combined_ground_truth.txt\"\n",
    "imagenet_val_dataset = ImageNet_Validation_DataSet(image_dir=imagenet_val_dir, combined_label_file=imagenet_val_combined_label_file, transformer=imagenet_transformer)\n",
    "imagenet_val_dataloader = data.DataLoader(imagenet_val_dataset, shuffle=False, batch_size=32, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f01e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    # transforms.RandomCrop((24, 24)),\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "cifar10_train_dataset = datasets.CIFAR10(\"../../../Datasets/CIFAR_10/\", train=True, download=True, transform=data_transform)\n",
    "cifar10_test_dataset = datasets.CIFAR10(\"../../../Datasets/CIFAR_10/\", train=False, download=True, transform=data_transform)\n",
    "\n",
    "cifar10_train_loader = data.DataLoader(cifar10_train_dataset, batch_size=32, shuffle=True)\n",
    "cifar10_test_loader = data.DataLoader(cifar10_test_dataset, batch_size=32)\n",
    "\n",
    "# cifar100_train_dataset = datasets.CIFAR100(\"./datasets\", train=True, download=True, transform=data_transform)\n",
    "# cifar100_test_dataset = datasets.CIFAR100(\"./datasets\", train=False, download=True, transform=data_transform)\n",
    "\n",
    "# cifar100_train_loader = data.DataLoader(cifar100_train_dataset, batch_size=32, shuffle=True, num_workers=os.cpu_count())\n",
    "# cifar100_test_loader = data.DataLoader(cifar100_test_dataset, batch_size=32, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d44077",
   "metadata": {},
   "source": [
    "## Building the model and Loading the Saved Weights\n",
    "## Defining the Mobilenetv1 Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe61828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MobileNetV1(load_tf_weights=True):\n",
    "\n",
    "    def ConvBatchReLU(\n",
    "            in_channels:int,\n",
    "            out_channels:int,\n",
    "            kernel_size:int,\n",
    "            stride:int = 1,\n",
    "            groups:int = 1,\n",
    "            pad:Tuple[int, int, int, int] = (0, 0, 0, 0),\n",
    "            bias=False,\n",
    "            eps=0.001, \n",
    "            momentum=0.01,\n",
    "    ):\n",
    "        return (Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, groups=groups, pad=pad, bias=bias),\n",
    "                BatchNorm2d(num_features=out_channels, eps=eps, momentum=momentum, affine=True, track_running_stats=True,),\n",
    "                ReLU6(inplace=True))\n",
    "    \n",
    "    def DepthwiseSeperableConv2LUBatchReLU(\n",
    "        in_channels:int,\n",
    "        out_channels:int,\n",
    "        kernel_size:int,\n",
    "        stride:int,\n",
    "        pad:Tuple[int, int, int, int] = (0, 0, 0, 0),\n",
    "        eps=0.001, \n",
    "        momentum=0.01\n",
    "    ):\n",
    "        return (\n",
    "            *ConvBatchReLU(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size, stride=stride, pad=pad, groups=in_channels, eps=eps, momentum=momentum),\n",
    "            *ConvBatchReLU(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, pad=(0,0,0,0), groups=1, eps=eps, momentum=momentum)\n",
    "        )\n",
    "    mobilenetv1 = {\n",
    "        \"conv2d_0\": [3, 32, 3, 2],\n",
    "\n",
    "        \"depthwiseseparable_0\": [32, 64, 3, 1],\n",
    "        \"depthwiseseparable_1\": [64, 128, 3, 2],\n",
    "        \"depthwiseseparable_2\": [128, 128, 3, 1],\n",
    "        \"depthwiseseparable_3\": [128, 256, 3, 2],\n",
    "        \"depthwiseseparable_4\": [256, 256, 3, 1],\n",
    "        \"depthwiseseparable_5\": [256, 512, 3, 2],\n",
    "\n",
    "        \"depthwiseseparable_6\": [512, 512, 3, 1],\n",
    "        \"depthwiseseparable_7\": [512, 512, 3, 1],\n",
    "        \"depthwiseseparable_8\": [512, 512, 3, 1],\n",
    "        \"depthwiseseparable_9\": [512, 512, 3, 1],\n",
    "        \"depthwiseseparable_10\": [512, 512, 3, 1],\n",
    "\n",
    "        \"depthwiseseparable_11\": [512, 1024, 3, 2],\n",
    "        \"depthwiseseparable_12\": [1024, 1024, 3, 1],\n",
    "\n",
    "        \"avgpool_0\": [7],\n",
    "        \"conv2d_1\": [1024, 1000, 1, 1],\n",
    "        \"flatten_0\": []\n",
    "    }\n",
    "\n",
    "    batchnorm_eps = 0.001\n",
    "    batchnorm_momentum = 1 - 0.99\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    for name, parameters in mobilenetv1.items():\n",
    "        if \"conv2d_0\" in name:\n",
    "            in_channels, out_channels, kernel_size, stride = parameters\n",
    "            if stride == 2:\n",
    "                pad = (0, 1, 0, 1)\n",
    "            else:\n",
    "                raise RuntimeError(f\"Unexpected type Conv layer\")\n",
    "            layers.extend(ConvBatchReLU(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, pad=pad, eps=batchnorm_eps, momentum=batchnorm_momentum))\n",
    "        elif \"conv2d_1\" in name:\n",
    "            in_channels, out_channels, kernel_size, stride = parameters\n",
    "            layers.append(Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride))\n",
    "        elif \"depthwiseseparable\" in name:\n",
    "            in_channels, out_channels, kernel_size, stride = parameters\n",
    "            if stride == 2:\n",
    "                pad = (0, 1, 0, 1)\n",
    "            else:\n",
    "                pad = tuple([1]*4)\n",
    "            layers.extend(DepthwiseSeperableConv2LUBatchReLU(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, pad=pad, eps=batchnorm_eps, momentum=batchnorm_momentum))\n",
    "        elif \"avgpool\" in name:\n",
    "            kernel_size = parameters[0]\n",
    "            layers.append(AvgPool2d(kernel_size=kernel_size))\n",
    "        elif \"flatten\" in name:\n",
    "            layers.append(Flatten())\n",
    "        else:\n",
    "            raise ValueError(f\"Recieved unexpected layer of {name}\")\n",
    "    mobilenetv1_model = Sequential(*layers)\n",
    "    if load_tf_weights:\n",
    "        mobilenetv1_model.load_state_dict(torch.load(mobilenetv1_state_dict_dmc_original_from_tf, weights_only=True), strict=True)\n",
    "    return mobilenetv1_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67ef2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "top1_acc_fun = lambda y_pred, y_true: ((y_pred).argmax(dim=1) == y_true).sum().item()    \n",
    "top5_acc_fun = lambda y_pred, y_true: (y_pred.topk(5, dim=1).indices == y_true.unsqueeze(1)).any(dim=1).sum().item()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f791d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'top1': <function <lambda> at 0x76dc93e7cf40>, 'top5': <function <lambda> at 0x76dc93e7cfe0>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [13:00<00:00,  2.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16927904, {'top1': 0.69042, 'top5': 88.508})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobilenetv1_model = MobileNetV1()\n",
    "mobilenetv1_model.to(DEVICE)\n",
    "\n",
    "original_results = mobilenetv1_model.evaluate(imagenet_val_dataloader, {\"top1\":top1_acc_fun, \"top5\":top5_acc_fun}, DEVICE)\n",
    "original_size = mobilenetv1_model.get_size_in_bytes()\n",
    "original_size, original_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59aea7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'top1': <function <lambda> at 0x76dc93e7cf40>, 'top5': <function <lambda> at 0x76dc93e7cfe0>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [13:37<00:00,  1.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16884128, {'top1': 0.69042, 'top5': 88.508})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobilenetv1_model_fused = MobileNetV1().fuse()\n",
    "mobilenetv1_model_fused.to(DEVICE)\n",
    "\n",
    "fused_results = mobilenetv1_model_fused.evaluate(imagenet_val_dataloader, {\"top1\":top1_acc_fun, \"top5\":top5_acc_fun}, DEVICE)\n",
    "fused_size = mobilenetv1_model_fused.get_size_in_bytes()\n",
    "fused_size, fused_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9b3a89",
   "metadata": {},
   "source": [
    "## Testing Compression on MobileNetV1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ee677e",
   "metadata": {},
   "source": [
    "### Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "934f7357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function <lambda> at 0x76dc93e7cf40>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      3\u001b[39m compression_config = {\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mprune_channel\u001b[39m\u001b[33m\"\u001b[39m :{\n\u001b[32m      5\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msparsity\u001b[39m\u001b[33m\"\u001b[39m : sp,\n\u001b[32m      6\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmetric\u001b[39m\u001b[33m\"\u001b[39m : \u001b[33m\"\u001b[39m\u001b[33ml2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m     },\n\u001b[32m      8\u001b[39m }\n\u001b[32m      9\u001b[39m compressed_mobilenetv1_mcu_model = mobilenetv1_model.init_compress(compression_config, input_shape=input_shape)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m before_acc1 = \u001b[43mcompressed_mobilenetv1_mcu_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimagenet_val_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop1_acc_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m*\u001b[32m100\u001b[39m\n\u001b[32m     12\u001b[39m before_acc5 = compressed_mobilenetv1_mcu_model.evaluate(imagenet_val_dataloader, top5_acc_fun, device=DEVICE)*\u001b[32m100\u001b[39m\n\u001b[32m     13\u001b[39m size = compressed_mobilenetv1_mcu_model.get_size_in_bytes()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/EmbeddedAI/deep-microcompression/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/EmbeddedAI/deep-microcompression/tests/mobilenetv1/../../development/models/sequential.py:303\u001b[39m, in \u001b[36mSequential.evaluate\u001b[39m\u001b[34m(self, data_loader, metrics, device)\u001b[39m\n\u001b[32m    301\u001b[39m \u001b[38;5;28mself\u001b[39m.eval()\n\u001b[32m    302\u001b[39m \u001b[38;5;28mprint\u001b[39m(metrics)\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m metric_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmetrics\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m():\n\u001b[32m    304\u001b[39m     metric_results[metric_name] = \u001b[32m0\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m X, y_true \u001b[38;5;129;01min\u001b[39;00m tqdm(data_loader):\n",
      "\u001b[31mAttributeError\u001b[39m: 'function' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "for i in range(0, 11, 1):\n",
    "    sp = i/10\n",
    "    compression_config = {\n",
    "        \"prune_channel\" :{\n",
    "            \"sparsity\" : sp,\n",
    "            \"metric\" : \"l2\"\n",
    "        },\n",
    "    }\n",
    "    compressed_mobilenetv1_mcu_model = mobilenetv1_model.init_compress(compression_config, input_shape=input_shape)\n",
    "\n",
    "    before_acc1 = compressed_mobilenetv1_mcu_model.evaluate(imagenet_val_dataloader, top1_acc_fun, device=DEVICE)*100\n",
    "    before_acc5 = compressed_mobilenetv1_mcu_model.evaluate(imagenet_val_dataloader, top5_acc_fun, device=DEVICE)*100\n",
    "    size = compressed_mobilenetv1_mcu_model.get_size_in_bytes()\n",
    "    print(f\"Before training, sparsity = {sp} acc1 = {before_acc1:.4f} acc5 = {before_acc5:.4f} size = {size/original_size*100:9.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb731f",
   "metadata": {},
   "source": [
    "### Dynamic Quantization Per Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48377c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [43:55<00:00,  1.69s/it]\n",
      "100%|██████████| 1563/1563 [44:22<00:00,  1.70s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m size = compressed_mobilenetv1_mcu_model.get_size_in_bytes()\n\u001b[32m     14\u001b[39m size = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBefore training, sparsity = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43msp\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m acc1 = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbefore_acc1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m acc5 = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbefore_acc5\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m size = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize/original_size*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m9.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'sp' is not defined"
     ]
    }
   ],
   "source": [
    "for b in [8, 4, 2]:\n",
    "    compression_config = {\n",
    "        \"quantize\" : {\n",
    "            \"scheme\" : QuantizationScheme.DYNAMIC,\n",
    "            \"granularity\": QuantizationGranularity.PER_TENSOR,\n",
    "            \"bitwidth\" : b\n",
    "        }\n",
    "    }\n",
    "    compressed_mobilenetv1_mcu_model = mobilenetv1_model.init_compress(compression_config, input_shape=input_shape)\n",
    "\n",
    "    before_acc1 = compressed_mobilenetv1_mcu_model.evaluate(imagenet_val_dataloader, top1_acc_fun, device=DEVICE)*100\n",
    "    before_acc5 = compressed_mobilenetv1_mcu_model.evaluate(imagenet_val_dataloader, top5_acc_fun, device=DEVICE)*100\n",
    "    size = compressed_mobilenetv1_mcu_model.get_size_in_bytes()\n",
    "    print(f\"Before training, bitwidth = {b} acc1 = {before_acc1:.4f} acc5 = {before_acc5:.4f} size = {size/original_size*100:9.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa8bc10",
   "metadata": {},
   "source": [
    "### Dynamic Quantization Per Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964b2d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in [8, 4, 2]:\n",
    "    compression_config = {\n",
    "        \"quantize\" : {\n",
    "            \"scheme\" : QuantizationScheme.DYNAMIC,\n",
    "            \"granularity\": QuantizationGranularity.PER_CHANNEL,\n",
    "            \"bitwidth\" : b\n",
    "        }\n",
    "    }\n",
    "    compressed_mobilenetv1_mcu_model = mobilenetv1_model.init_compress(compression_config, input_shape=input_shape)\n",
    "\n",
    "    before_acc1 = compressed_mobilenetv1_mcu_model.evaluate(imagenet_val_dataloader, top1_acc_fun, device=DEVICE)*100\n",
    "    before_acc5 = compressed_mobilenetv1_mcu_model.evaluate(imagenet_val_dataloader, top5_acc_fun, device=DEVICE)*100\n",
    "    size = compressed_mobilenetv1_mcu_model.get_size_in_bytes()\n",
    "    print(f\"Before training, bitwidth = {b} acc1 = {before_acc1:.4f} acc5 = {before_acc5:.4f} size = {size/original_size*100:9.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3df2bc",
   "metadata": {},
   "source": [
    "### Static Quantization Per Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b0495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in [8, 4, 2]:\n",
    "    compression_config = {\n",
    "        \"quantize\" : {\n",
    "            \"scheme\" : QuantizationScheme.STATIC,\n",
    "            \"granularity\": QuantizationGranularity.PER_TENSOR,\n",
    "            \"bitwidth\" : b\n",
    "        }\n",
    "    }\n",
    "    compressed_mobilenetv1_mcu_model = mobilenetv1_model.init_compress(compression_config, input_shape=input_shape, calibration_data=next(iter(imagenet_val_dataloader))[0].to(DEVICE))\n",
    "\n",
    "    before_acc1 = compressed_mobilenetv1_mcu_model.evaluate(imagenet_val_dataloader, top1_acc_fun, device=DEVICE)*100\n",
    "    before_acc5 = compressed_mobilenetv1_mcu_model.evaluate(imagenet_val_dataloader, top5_acc_fun, device=DEVICE)*100\n",
    "    size = compressed_mobilenetv1_mcu_model.get_size_in_bytes()\n",
    "    print(f\"Before training, bitwidth = {b} acc1 = {before_acc1:.4f} acc5 = {before_acc5:.4f} size = {size/original_size*100:9.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993b9d4c",
   "metadata": {},
   "source": [
    "### Static Quantization Per Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c1ff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in [8, 4, 2]:\n",
    "    compression_config = {\n",
    "        \"quantize\" : {\n",
    "            \"scheme\" : QuantizationScheme.STATIC,\n",
    "            \"granularity\": QuantizationGranularity.PER_CHANNEL,\n",
    "            \"bitwidth\" : b\n",
    "        }\n",
    "    }\n",
    "    compressed_mobilenetv1_mcu_model = mobilenetv1_model.init_compress(compression_config, input_shape=input_shape, calibration_data=next(iter(imagenet_val_dataloader))[0].to(DEVICE))\n",
    "\n",
    "    before_acc1 = compressed_mobilenetv1_mcu_model.evaluate(imagenet_val_dataloader, top1_acc_fun, device=DEVICE)*100\n",
    "    before_acc5 = compressed_mobilenetv1_mcu_model.evaluate(imagenet_val_dataloader, top5_acc_fun, device=DEVICE)*100\n",
    "    size = compressed_mobilenetv1_mcu_model.get_size_in_bytes()\n",
    "    print(f\"Before training, bitwidth = {b} acc1 = {before_acc1:.4f} acc5 = {before_acc5:.4f} size = {size/original_size*100:9.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3238f2",
   "metadata": {},
   "source": [
    "# MobileNetV1 Tensorflow to Torch Converter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98babdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m mobilenetv1_tf_model = \u001b[43mtf\u001b[49m.keras.applications.MobileNet(weights=\u001b[33m\"\u001b[39m\u001b[33mimagenet\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# [layer for layer in tf_model.layers if layer.weights]\u001b[39;00m\n\u001b[32m      3\u001b[39m torch_layers = []\n",
      "\u001b[31mNameError\u001b[39m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def convert_mobilenetv1_tf_to_torch(save_weight=True):\n",
    "\n",
    "    def copy_tensor(tensor_source, tensor_destination):\n",
    "        tensor_destination.copy_(tensor_source)\n",
    "\n",
    "    mobilenetv1_tf_model = tf.keras.applications.MobileNet(weights=\"imagenet\")\n",
    "    torch_layers = []\n",
    "\n",
    "    for i, layer in enumerate(mobilenetv1_tf_model.layers):\n",
    "    \n",
    "        if isinstance(layer, tf.keras.layers.InputLayer):\n",
    "            pass\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.Conv2D):\n",
    "            # print(layer.weights[0].shape)\n",
    "\n",
    "            weight = np.transpose(layer.weights[0], (3, 2, 0, 1))\n",
    "            out_channels, in_channels, kernel_size, _ = weight.shape\n",
    "            stride =layer.strides[0]\n",
    "\n",
    "            if kernel_size == 3 and stride == 2:\n",
    "                pad = (0, 1, 0, 1)\n",
    "                padding = 0\n",
    "            elif kernel_size == 1 and stride == 1:\n",
    "                pad = (0,0,0,0)\n",
    "                padding = 0\n",
    "            else:\n",
    "                raise RuntimeError(f\"Unexpected type Conv layer\")\n",
    "\n",
    "            # torch_layers.append(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=layer.strides[0], padding=1 bias=True))\n",
    "            torch_layers.append(Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "            kernel_size=kernel_size, stride=stride, padding=padding, pad=pad, \n",
    "            bias=len(layer.weights)==2))\n",
    "            copy_tensor(torch.from_numpy(weight), torch_layers[-1].weight)\n",
    "                \n",
    "            if len(layer.weights) == 2:\n",
    "                # nn.init.constant_(torch_layers[-1].bias, 0.0)\n",
    "            # else:\n",
    "                copy_tensor(torch.from_numpy(layer.weights[1].numpy()), torch_layers[-1].bias)\n",
    "\n",
    "            pass\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.DepthwiseConv2D):\n",
    "            # Convert to PyTorch format: [in_channels, 1, H, W]\n",
    "            weight = np.transpose(layer.weights[0], (2, 3, 0, 1))\n",
    "            \n",
    "            in_channels, depth_multiplier, kernel_size, _ = weight.shape\n",
    "            stride =layer.strides[0]\n",
    "\n",
    "            assert depth_multiplier == 1, \"Depth multiplier must be 1 for depthwise convolutions\"\n",
    "\n",
    "            if kernel_size == 3 and stride == 2:\n",
    "                padding = 0\n",
    "                pad = (0, 1, 0, 1)\n",
    "            elif kernel_size == 3 and stride == 1:\n",
    "                padding = 0\n",
    "                pad = (1,1,1,1)\n",
    "            else:\n",
    "                raise RuntimeError(f\"Unexpected type Conv layer\")\n",
    "\n",
    "            # torch_layers.append(nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size, stride=layer.strides[0], padding=1, groups=in_channels, bias=True))\n",
    "            torch_layers.append(\n",
    "        Conv2d(in_channels=in_channels, out_channels=in_channels, \n",
    "        kernel_size=kernel_size, stride=stride, padding=padding, pad=pad, \n",
    "        groups=in_channels, bias=len(layer.weights)==2))\n",
    "            print(i, weight.shape, torch_layers[-1].weight.shape, in_channels)\n",
    "\n",
    "            # Copy weights\n",
    "            copy_tensor(torch.from_numpy(weight), torch_layers[-1].weight)\n",
    "            # nn.init.constant_(torch_layers[-1].bias, 0.0)\n",
    "            pass\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.ReLU):\n",
    "            # torch_layers.append(nn.ReLU())\n",
    "            torch_layers.append(ReLU6())\n",
    "            pass\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            gamma, beta, mean, var = layer.weights\n",
    "            out_channels = gamma.shape[0]\n",
    "\n",
    "            # torch_layers.append(nn.BatchNorm2d(out_channels, eps=layer.epsilon, momentum=layer.momentum))\n",
    "            torch_layers.append(BatchNorm2d(out_channels, eps=layer.epsilon, momentum=1 - layer.momentum, affine=layer.center and layer.scale,  track_running_stats=True,))\n",
    "            # print(torch_layers[-1].momentum)  \n",
    "            # print(layer.epsilon, layer.momentum, layer.center, layer.scale)\n",
    "            copy_tensor(torch.from_numpy(gamma.numpy()), torch_layers[-1].weight)\n",
    "            copy_tensor(torch.from_numpy(beta.numpy()), torch_layers[-1].bias)\n",
    "            copy_tensor(torch.from_numpy(mean.numpy()), torch_layers[-1].running_mean)\n",
    "            copy_tensor(torch.from_numpy(var.numpy()), torch_layers[-1].running_var)\n",
    "\n",
    "            # print(torch_layers[-1].running_var, layer.weights)\n",
    "\n",
    "            # print(var)\n",
    "            # print(layer.moving_variance)\n",
    "\n",
    "            # break\n",
    "            pass\n",
    "            \n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            weight, bias = layer.weights\n",
    "            # torch_layers.append(nn.Linear(weight.shape[0], weight.shape[1]))\n",
    "            torch_layers.append(Linear(weight.shape[0], weight.shape[1]))\n",
    "\n",
    "            copy_tensor(torch.from_numpy(weight.numpy().T), torch_layers[-1].weight)\n",
    "            copy_tensor(torch.from_numpy(bias.numpy()), torch_layers[-1].bias)\n",
    "\n",
    "            # print(weight, bias)\n",
    "            pass\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.GlobalAveragePooling2D):\n",
    "            torch_layers.append(AvgPool2d(kernel_size=7))\n",
    "            # torch_layers.append(MaxPool2d(kernel_size=7))\n",
    "            pass\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.ZeroPadding2D):\n",
    "            pass\n",
    "        elif isinstance(layer, tf.keras.layers.Dropout):\n",
    "            pass\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.Reshape):\n",
    "            # print(\"Not Needed\")\n",
    "            # torch_layers.append(Flatten())\n",
    "            pass\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.Activation):\n",
    "            # torch_layers.append(nn.Softmax(dim=1))\n",
    "            pass\n",
    "\n",
    "        else: \n",
    "            pass\n",
    "            raise RuntimeError(f\"Unknown layer type: {type(layer)}\")\n",
    "    mobilenetv1_model = Sequential(*torch_layers)\n",
    "    if save_weight:\n",
    "        torch.save(mobilenetv1_model.state_dict(), mobilenetv1_state_dict_dmc_original_from_tf)\n",
    "    return mobilenetv1_model\n",
    "    # print(layer.name, type(layer))\n",
    "mobilenetv1_torch_model = convert_mobilenetv1_tf_to_torch(save_weight=False)\n",
    "type(mobilenetv1_torch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b47d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
